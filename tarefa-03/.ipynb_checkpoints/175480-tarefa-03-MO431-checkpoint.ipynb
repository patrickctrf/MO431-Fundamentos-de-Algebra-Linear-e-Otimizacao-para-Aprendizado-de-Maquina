{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 03 - MO431\n",
    "\n",
    "## Patrick de Carvalho Tavares Rezende Ferreira - 175480\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pybobyqa\n",
    "from scipy.optimize import minimize, line_search\n",
    "from numpy import array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo definimos a função objetivo a ser minimizada (função de himmelblau) e seu gradiente. A função recebe o valr do ponto (x, y) a ser avaliado e retorna um float com o valor real da função naquele ponto, enquanto que o gradiente também recebe um ponto onde será avaliado e retorna um vetor que é o próprio gradiente da função naquele ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x):\n",
    "    # Garantindo que trabalhamos com array numpy, e nao uma lista\n",
    "    x = array(x)\n",
    "\n",
    "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "\n",
    "\n",
    "def grad_himmelblau(x, *args):\n",
    "    # Para x_0\n",
    "    dx_0 = 2 * (2 * x[0]*(x[0] ** 2 + x[1] - 11) + x[0] + x[1] ** 2 - 7)\n",
    "\n",
    "    # Para x_1\n",
    "    dx_1 = 2 * (x[0] ** 2 + 2 * x[1] * (x[0] + x[1] ** 2 - 7) + x[1] - 11)\n",
    "\n",
    "    return array([dx_0, dx_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo executamos as 5 técnicas de minimização solicitadas pelo roteiro e, em seguida, ocorre a discussão acerca dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRADIENTE-CONJUGADO\n",
      "\n",
      "iterações:  8\n",
      "chamadas do gradiente:  68\n",
      "x:  [2.99999999 2.        ]\n",
      "himmelblau(x):  1.9486292751344026e-15\n",
      "Tempo demandado pela otimização [s]:  0.0020203590393066406\n",
      "\n",
      "\n",
      "\n",
      "L-BFGS-B-sem-grad\n",
      "\n",
      "iterações:  9\n",
      "chamadas do gradiente:  30\n",
      "x:  [2.99999985 2.00000019]\n",
      "himmelblau(x):  8.502778926721376e-13\n",
      "Tempo demandado pela otimização [s]:  0.0015976428985595703\n",
      "\n",
      "\n",
      "\n",
      "L-BFGS-B-com-grad\n",
      "\n",
      "iterações:  9\n",
      "chamadas do gradiente:  10\n",
      "x:  [2.99999986 2.00000019]\n",
      "himmelblau(x):  8.287611020495648e-13\n",
      "Tempo demandado pela otimização [s]:  0.0007684230804443359\n",
      "\n",
      "\n",
      "\n",
      "Nelder-mead\n",
      "\n",
      "iterações:  40\n",
      "avaliações dos vértices do triângulo:  77\n",
      "x:  [ 3.58441449 -1.84811588]\n",
      "himmelblau(x):  1.0686566996168641e-08\n",
      "Tempo demandado pela otimização [s]:  0.0035278797149658203\n",
      "\n",
      "\n",
      "\n",
      "Line-Search começando na direção [-1,-1]\n",
      "\n",
      "iterações:  13\n",
      "chamadas do gradiente:  0\n",
      "x:  [2.5 2.5]\n",
      "himmelblau(x):  8.125\n",
      "Tempo demandado pela otimização [s]:  0.0013186931610107422\n",
      "\n",
      "\n",
      "\n",
      "Line-Search começando na direção oposta ao gradiente\n",
      "\n",
      "iterações:  13\n",
      "chamadas do gradiente:  0\n",
      "x:  [-2.36228496 -4.45809648]\n",
      "himmelblau(x):  208.07835720768088\n",
      "Tempo demandado pela otimização [s]:  0.001665353775024414\n",
      "\n",
      "\n",
      "\n",
      "BOBYQA\n",
      "****** Py-BOBYQA Results ******\n",
      "Solution xmin = [3. 2.]\n",
      "Objective value f(xmin) = 1.287703555e-21\n",
      "Needed 58 objective evaluations (at 58 points)\n",
      "Approximate gradient = [6.40527549e-09 5.63832659e-08]\n",
      "Approximate Hessian = [[73.79866528 20.17722326]\n",
      " [20.17722326 34.16328251]]\n",
      "Exit flag = 0\n",
      "Success: rho has reached rhoend\n",
      "******************************\n",
      "\n",
      "Tempo demandado pela otimização [s]:  0.10146498680114746\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =====GRADIENTE-CONJUGADO==================================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"CG\", jac=None)\n",
    "tf = time.time()\n",
    "print(\"\\nGRADIENTE-CONJUGADO\")\n",
    "print(\"\\niterações: \", val.nit)\n",
    "print(\"chamadas do gradiente: \", val.nfev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ======BFGS-SEM-GRADIENTE-PASSADO==========================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"L-BFGS-B\", jac=None)\n",
    "tf = time.time()\n",
    "print(\"\\nL-BFGS-B-sem-grad\")\n",
    "print(\"\\niterações: \", val.nit)\n",
    "print(\"chamadas do gradiente: \", val.nfev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ======BFGS-COM-GRADIENTE-PASSADO==========================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"L-BFGS-B\", jac=grad_himmelblau)\n",
    "tf = time.time()\n",
    "print(\"\\nL-BFGS-B-com-grad\")\n",
    "print(\"\\niterações: \", val.nit)\n",
    "print(\"chamadas do gradiente: \", val.nfev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ==========NELDER-MEAN=====================================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"Nelder-Mead\", options={'initial_simplex':array([[-4, -4], [-4, 1], [4, -1]])})\n",
    "tf = time.time()\n",
    "print(\"\\nNelder-mead\")\n",
    "print(\"\\niterações: \", val.nit)\n",
    "print(\"avaliações dos vértices do triângulo: \", val.nfev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "# =======LINE-SEARCH========================================================\n",
    "x = array([4, 4])\n",
    "x_new = x.copy()\n",
    "pk = array([-1, -1]) # -grad_himmelblau(x_new)\n",
    "\n",
    "t0 = time.time()\n",
    "while(1):\n",
    "    alpha, fc, gc, new_fval, old_fval, new_slope = line_search(himmelblau, grad_himmelblau, x_new, pk=pk)\n",
    "\n",
    "    # Se ainda nao convergiu, continue iterando\n",
    "    if alpha is not None:\n",
    "        x_new = x_new + alpha * pk\n",
    "    else:\n",
    "        # Quando convergir, sai do loop\n",
    "        break\n",
    "tf = time.time()\n",
    "print(\"\\nLine-Search começando na direção [-1,-1]\")\n",
    "print(\"\\niterações: \", fc)\n",
    "print(\"chamadas do gradiente: \", gc)\n",
    "print(\"x: \", x_new)\n",
    "print(\"himmelblau(x): \", old_fval)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "\n",
    "x = array([4, 4])\n",
    "x_new = x.copy()\n",
    "pk = -grad_himmelblau(x_new)\n",
    "\n",
    "t0 = time.time()\n",
    "while(1):\n",
    "    alpha, fc, gc, new_fval, old_fval, new_slope = line_search(himmelblau, grad_himmelblau, x_new, pk=pk)\n",
    "\n",
    "    # Se ainda nao convergiu, continue iterando\n",
    "    if alpha is not None:\n",
    "        x_new = x_new + alpha * pk\n",
    "    else:\n",
    "        # Quando convergir, sai do loop\n",
    "        break\n",
    "tf = time.time()\n",
    "print(\"\\nLine-Search começando na direção oposta ao gradiente\")\n",
    "print(\"\\niterações: \", fc)\n",
    "print(\"chamadas do gradiente: \", gc)\n",
    "print(\"x: \", x_new)\n",
    "print(\"himmelblau(x): \", old_fval)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n\")\n",
    "\n",
    "# ===========BOBYQA=========================================================\n",
    "x = array([4, 4])\n",
    "print(\"\\nBOBYQA\")\n",
    "# Estabelecendo os limites (lower <= x <= upper)\n",
    "lower = array([-10.0, -10.0])\n",
    "upper = array([10.0, 10.0])\n",
    "# Executa a minimizacao\n",
    "t0 = time.time()\n",
    "val = pybobyqa.solve(himmelblau, x, bounds=(lower, upper))\n",
    "tf = time.time()\n",
    "\n",
    "# Imprime resultados\n",
    "print(val)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussão dos Resultados\n",
    "\n",
    "As saídas das 5 técnicas de otimização (Conjugado do gradiente, busca em linha, Nelder Mead, BFGS, BOBYQA) para a função de Himmelblau são apresentadas acima.\n",
    "\n",
    "A menor quantidade de steps (iterações) necessários à otimização foi para a técnica de gradiente conjugado, como era de se esperar, já que é uma técnica de passo grande e, se for aplicada adequadamente, converge de maneira rápida. Embora tenha feito menos iterações que o BFGS, seu tempo ainda foi pior, devido às diversas chamadas do gradiente, sendo de 0.005189180374145508s contra aproximadamente 0.00168s para ambos os BFGS.\n",
    "\n",
    "O segundo método que precisou de menos iterações foi o BFGS, que é um método quadrático e realizou menos consultas ao gradiente que o próprio gradiente conjugado, sendo de 30 para o gradiente nao passado (cáculo implícito na função) e 10 para o gradiente passado analiticamente. Além disso, o valor de mínimo local encontrado foi menor que o do gradiente conjugado, o que indica que a aproximação por uma função quadrática naquele local parece ter sido uma adequada abordagem.\n",
    "\n",
    "O line-search fez 13 iterações, mas nenhuma consulta ao gradiente, tendosido mais rápido que o gradiente conjugado para encontrar o mínimo local, com 0.002579212188720703s. Entretando, a qualidade do mínimo encontrada por este método é inferior, sendo de valor 8.125 quando iniciamos a busca na direção (-1, -1) e de 208.07835720768088 se começarmos na direção em que o gradiente aponta na posição inicial (não convergindo o algoritmo). \n",
    "\n",
    "Enquanto que BFGS e Graidiente conjugado convergiram para o mínimo em [x=2.99999985, y=2.00000019], o método de Nelder-Mead encontrou um mínimo em [x=3.58441449, y=-1.84811588] com valor de 1.0686566996168641e-08. Com muitas iterações, cálculos dos vértices do triângulo e um tempo de execução de 0.007497310638427734s, fica claro que este método é recomendável principalmente para casos em que o cálculo do gradiente seja altamente custoso por algum motivo.\n",
    "\n",
    "Com 58 avaliações da função objetivo e um tempo de execução de 0.10592770576477051s (o mais lento deste experimento), o método NEWOA ou BOBYQA foi o que encontrou o melhor mínimo local, sendo de 1.287703555e-21, no ponto [x=3. y=2.] e sem a necessidade de cálculo do gradiente. Isto indica que este método é uma boa alternativa a funções com um gradiente custoso computacionalmente e que produz bons resultados, embora deva-se levar em consideração que seu tempo de execução é alto.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
