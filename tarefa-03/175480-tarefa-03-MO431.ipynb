{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarefa 03 - MO431\n",
    "\n",
    "## Patrick de Carvalho Tavares Rezende Ferreira - 175480\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import pybobyqa\n",
    "from scipy.optimize import minimize, line_search\n",
    "from numpy import array\n",
    "from numpy.random import seed\n",
    "\n",
    "seed(1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo definimos a função objetivo a ser minimizada (função de himmelblau) e seu gradiente. A função recebe o valr do ponto (x, y) a ser avaliado e retorna um float com o valor real da função naquele ponto, enquanto que o gradiente também recebe um ponto onde será avaliado e retorna um vetor que é o próprio gradiente da função naquele ponto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def himmelblau(x):\n",
    "    # Garantindo que trabalhamos com array numpy, e nao uma lista\n",
    "    x = array(x)\n",
    "\n",
    "    return (x[0] ** 2 + x[1] - 11) ** 2 + (x[0] + x[1] ** 2 - 7) ** 2\n",
    "\n",
    "\n",
    "def grad_himmelblau(x, *args):\n",
    "    # Para x_0\n",
    "    dx_0 = 2 * (2 * x[0]*(x[0] ** 2 + x[1] - 11) + x[0] + x[1] ** 2 - 7)\n",
    "\n",
    "    # Para x_1\n",
    "    dx_1 = 2 * (x[0] ** 2 + 2 * x[1] * (x[0] + x[1] ** 2 - 7) + x[1] - 11)\n",
    "\n",
    "    return array([dx_0, dx_1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abaixo executamos as 5 técnicas de minimização solicitadas pelo roteiro e, em seguida, ocorre a discussão acerca dos resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "GRADIENTE-CONJUGADO sem passagem de gradiente\n",
      "\n",
      "quantidade de chamadas da função objetivo:  68\n",
      "chamadas do gradiente:  17\n",
      "x:  [2.99999999 2.        ]\n",
      "himmelblau(x):  1.9486292751344026e-15\n",
      "Tempo demandado pela otimização [s]:  0.0019459724426269531\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "GRADIENTE-CONJUGADO com gradiente analítico\n",
      "\n",
      "quantidade de chamadas da função objetivo:  17\n",
      "chamadas do gradiente:  17\n",
      "x:  [3. 2.]\n",
      "himmelblau(x):  8.490750396096654e-21\n",
      "Tempo demandado pela otimização [s]:  0.0025606155395507812\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "L-BFGS-B-sem-grad\n",
      "\n",
      "quantidade de chamadas da função objetivo:  30\n",
      "chamadas do gradiente:  9\n",
      "x:  [2.99999985 2.00000019]\n",
      "himmelblau(x):  8.502778926721376e-13\n",
      "Tempo demandado pela otimização [s]:  0.0009605884552001953\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "L-BFGS-B-com-grad\n",
      "\n",
      "quantidade de chamadas da função objetivo:  10\n",
      "chamadas do gradiente:  9\n",
      "x:  [2.99999986 2.00000019]\n",
      "himmelblau(x):  8.287611020495648e-13\n",
      "Tempo demandado pela otimização [s]:  0.0010695457458496094\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "Nelder-mead\n",
      "\n",
      "quantidade de chamadas da função objetivo:  76\n",
      "avaliações dos vértices do triângulo:  40\n",
      "x:  [3.00002182 1.99995542]\n",
      "himmelblau(x):  3.19442883153528e-08\n",
      "Tempo demandado pela otimização [s]:  0.004319667816162109\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "Line-Search começando na direção oposta ao gradiente\n",
      "\n",
      "quantidade de chamadas da função objetivo:  208\n",
      "chamadas do gradiente:  29\n",
      "x:  [-3.77931025 -3.28318599]\n",
      "himmelblau(x):  7.888609052210118e-31\n",
      "Tempo demandado pela otimização [s]:  0.008239984512329102\n",
      "\n",
      "-------------------------------------------------\n",
      "\n",
      "\n",
      "BOBYQA\n",
      "****** Py-BOBYQA Results ******\n",
      "Solution xmin = [3. 2.]\n",
      "Objective value f(xmin) = 1.287703555e-21\n",
      "Needed 58 objective evaluations (at 58 points)\n",
      "Approximate gradient = [6.40527549e-09 5.63832659e-08]\n",
      "Approximate Hessian = [[73.79866528 20.17722326]\n",
      " [20.17722326 34.16328251]]\n",
      "Exit flag = 0\n",
      "Success: rho has reached rhoend\n",
      "******************************\n",
      "\n",
      "Tempo demandado pela otimização [s]:  0.10332751274108887\n",
      "\n",
      "-------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/patrick/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:56: OptimizeWarning: Unknown solver options: .nfevial_simplex\n"
     ]
    }
   ],
   "source": [
    "# =====GRADIENTE-CONJUGADO-sem-passagem-de-gradiente========================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"CG\", jac=None)\n",
    "tf = time.time()\n",
    "print(\"\\nGRADIENTE-CONJUGADO sem passagem de gradiente\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", val.nfev)\n",
    "print(\"chamadas do gradiente: \", val.njev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# =====GRADIENTE-CONJUGADO-com-passagem-de-gradiente========================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"CG\", jac=grad_himmelblau)\n",
    "tf = time.time()\n",
    "print(\"\\nGRADIENTE-CONJUGADO com gradiente analítico\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", val.nfev)\n",
    "print(\"chamadas do gradiente: \", val.njev)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# ======BFGS-SEM-GRADIENTE-PASSADO==========================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"L-BFGS-B\", jac=None)\n",
    "tf = time.time()\n",
    "print(\"\\nL-BFGS-B-sem-grad\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", val.nfev)\n",
    "print(\"chamadas do gradiente: \", val.nit)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# ======BFGS-COM-GRADIENTE-PASSADO==========================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"L-BFGS-B\", jac=grad_himmelblau)\n",
    "tf = time.time()\n",
    "print(\"\\nL-BFGS-B-com-grad\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", val.nfev)\n",
    "print(\"chamadas do gradiente: \", val.nit)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# ==========NELDER-MEAN=====================================================\n",
    "x = array([4, 4])\n",
    "t0 = time.time()\n",
    "val = minimize(himmelblau, x, method=\"Nelder-Mead\", options={'.nfevial_simplex':array([[-4, -4], [-4, 1], [4, -1]])})\n",
    "tf = time.time()\n",
    "print(\"\\nNelder-mead\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", val.nfev)\n",
    "print(\"avaliações dos vértices do triângulo: \", val.nit)\n",
    "print(\"x: \", val.x)\n",
    "print(\"himmelblau(x): \", val.fun)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# =======LINE-SEARCH========================================================\n",
    "x = array([4, 4])\n",
    "x_new = x.copy()\n",
    "pk = -grad_himmelblau(x_new)\n",
    "gc_counter = 0\n",
    "fc_counter = 0\n",
    "\n",
    "t0 = time.time()\n",
    "while (1):\n",
    "    alpha, fc, gc, new_fval, old_fval, new_slope = line_search(himmelblau, grad_himmelblau, x_new, pk=pk)\n",
    "    fc_counter += fc\n",
    "    gc_counter += gc\n",
    "\n",
    "    # Se ainda nao convergiu, continue iterando\n",
    "    if alpha is not None:\n",
    "        x_new = x_new + alpha * pk\n",
    "        pk = -new_slope\n",
    "    else:\n",
    "        # Quando convergir, sai do loop\n",
    "        break\n",
    "tf = time.time()\n",
    "print(\"\\nLine-Search começando na direção oposta ao gradiente\")\n",
    "print(\"\\nquantidade de chamadas da função objetivo: \", fc_counter)\n",
    "print(\"chamadas do gradiente: \", gc_counter)\n",
    "print(\"x: \", x_new)\n",
    "print(\"himmelblau(x): \", old_fval)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n",
    "# ===========BOBYQA=========================================================\n",
    "x = array([4, 4])\n",
    "print(\"\\nBOBYQA\")\n",
    "# Estabelecendo os limites (lower <= x <= upper)\n",
    "lower = array([-10.0, -10.0])\n",
    "upper = array([10.0, 10.0])\n",
    "# Executa a minimizacao\n",
    "t0 = time.time()\n",
    "val = pybobyqa.solve(himmelblau, x, bounds=(lower, upper))\n",
    "tf = time.time()\n",
    "\n",
    "# Imprime resultados\n",
    "print(val)\n",
    "print(\"Tempo demandado pela otimização [s]: \", tf - t0)\n",
    "print(\"\\n-------------------------------------------------\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discussão dos Resultados\n",
    "\n",
    "As saídas das 5 técnicas de otimização (Conjugado do gradiente, busca em linha, Nelder Mead, BFGS, BOBYQA) para a função de Himmelblau são apresentadas acima.\n",
    "\n",
    "A menor quantidade de steps (iterações) necessários à otimização foi para a técnica de gradiente conjugado, como era de se esperar, já que é uma técnica de passo grande e, se for aplicada adequadamente, converge de maneira rápida. Embora tenha feito menos iterações que o BFGS, seu tempo ainda foi pior, devido às diversas chamadas do gradiente, sendo de 0.0025606155395507812 contra aproximadamente 0.0010s para ambos os BFGS.\n",
    "\n",
    "O gradiente conjugado com passagem de gradiente analítico explícito não foi solicitado e foi calculado apenas a título de informação, tendo desempenhado menos consultas à função objetivo do que o método com passagem explícita, mas demorando aproximadamente o mesmo tempo. O mínimo encontrado, entretando, foi menor e calculado em outro ponto, como exibido acima. Neste relatório, quando cita-se gradiente conjugado, estaremos falando daquele sem passagem de gradiente explícita, conforme solictao no roteiro.\n",
    "\n",
    "O segundo método que precisou de menos iterações foi o BFGS, que é um método quadrático e realizou menos consultas à função objetivo que o próprio gradiente conjugado, sendo de 30 para o gradiente nao passado (cáculo do gradiente implícito na função) e 10 para o gradiente passado analiticamente. Além disso, o valor de mínimo local encontrado foi menor que o do gradiente conjugado, o que indica que a aproximação por uma função quadrática naquele local parece ter sido uma adequada abordagem. Tanto o método com passagem como o sem passagem de gradiente produziram resultados muito próximos de mínimo local e aproximadamente mesmo tempo computacional.\n",
    "\n",
    "O line-search fez 208 chamadas à função objetivo e 29 ao gradiente, tendo sido mais lento que o gradiente conjugado para encontrar o mínimo local, com 0.008239984512329102. Entretando, a qualidade do mínimo encontrada por este método é superior, sendo de valor 7.888609052210118e-31 no ponto [x=-3.77931025, y=-3.28318599].\n",
    "\n",
    "Enquanto que BFGS e Gradiente conjugado convergiram para o mínimo em [x=2.99999985, y=2.00000019], o método de Nelder-Mead encontrou um mínimo em [x=3.58441449, y=-1.84811588] com valor de 1.0686566996168641e-08. Com muitas iterações, cálculos dos vértices do triângulo e um tempo de execução de 0.007497310638427734s, fica claro que este método é recomendável principalmente para casos em que o cálculo do gradiente seja altamente custoso por algum motivo.\n",
    "\n",
    "Com 58 avaliações da função objetivo e um tempo de execução de 0.10332751274108887s (o mais lento deste experimento), o método NEWOA ou BOBYQA foi o que encontrou o segundo melhor mínimo local, sendo de 1.287703555e-21, no ponto [x=3. y=2.] e sem a necessidade de cálculo do gradiente. Isto indica que este método é uma boa alternativa a funções com um gradiente custoso computacionalmente e que produz bons resultados, embora deva-se levar em consideração que seu tempo de execução é alto.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
